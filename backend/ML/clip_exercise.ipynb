{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평범한 연습장입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "소파 와 고양이 tensor(0.7725, grad_fn=<UnbindBackward0>)\n",
      "자동차 tensor(0.0967, grad_fn=<UnbindBackward0>)\n",
      "뭘봐 tensor(0.0662, grad_fn=<UnbindBackward0>)\n",
      "쳇바퀴를 달리는 햄스터 tensor(0.0609, grad_fn=<UnbindBackward0>)\n",
      "강아지와 강아지 주인 tensor(0.0037, grad_fn=<UnbindBackward0>)\n",
      "tensor([[ 0.0799,  0.0500, -0.0342,  ...,  0.0703,  0.0370,  0.0331],\n",
      "        [-0.0421, -0.0086, -0.0381,  ..., -0.0666, -0.0429, -0.0286],\n",
      "        [ 0.0690,  0.0496, -0.0178,  ..., -0.0152, -0.0364,  0.0090],\n",
      "        [ 0.0620,  0.0566, -0.0062,  ..., -0.0022, -0.0360,  0.0009],\n",
      "        [-0.0601,  0.0624, -0.0363,  ...,  0.0032, -0.0540, -0.0414]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "processor = AutoProcessor.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "model = AutoModel.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = [\"소파 와 고양이\", \"강아지와 강아지 주인\", \"쳇바퀴를 달리는 햄스터\", \"자동차\", \"뭘봐\"]\n",
    "image\n",
    "#Load CLIP model\n",
    "# model = SentenceTransformer('clip-ViT-B-32')\n",
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "inputs = processor(\n",
    "    text=text,\n",
    "    images=image, \n",
    "    return_tensors=\"pt\", # could also be \"pt\" \n",
    "    padding=True\n",
    ")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "probs = softmax(outputs.logits_per_image)\n",
    "\n",
    "for idx, prob in sorted(enumerate(*probs), key=lambda x: x[1], reverse=True):\n",
    "    print(text[idx], prob)\n",
    "    \n",
    "te = outputs.text_embeds\n",
    "ie = outputs.image_embeds\n",
    "\n",
    "#Encode an image:\n",
    "# img_emb = model.encode(Image.open('two_dogs_in_snow.png')) # or jpg\n",
    "\n",
    "#Encode text descriptions\n",
    "# text_emb = model.encode(['두 개가 눈 위에 있다.', '고양이가 책상에 앉아있다.', '저녁 런던의 사진'])\n",
    "\n",
    "#Compute cosine similarities \n",
    "# cos_scores = util.cos_sim(img_emb, text_emb)\n",
    "# print(cos_scores)\n",
    "print(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "자동차 tensor(0.3735, grad_fn=<UnbindBackward0>)\n",
      "강아지와 강아지 주인 tensor(0.2243, grad_fn=<UnbindBackward0>)\n",
      "소파 와 고양이 tensor(0.2202, grad_fn=<UnbindBackward0>)\n",
      "쳇바퀴를 달리는 햄스터 tensor(0.1324, grad_fn=<UnbindBackward0>)\n",
      "뭘봐 tensor(0.0496, grad_fn=<UnbindBackward0>)\n",
      "tensor([[ 0.0799,  0.0500, -0.0342,  ...,  0.0703,  0.0370,  0.0331],\n",
      "        [-0.0421, -0.0086, -0.0381,  ..., -0.0666, -0.0429, -0.0286],\n",
      "        [ 0.0690,  0.0496, -0.0178,  ..., -0.0152, -0.0364,  0.0090],\n",
      "        [ 0.0620,  0.0566, -0.0062,  ..., -0.0022, -0.0360,  0.0009],\n",
      "        [-0.0601,  0.0624, -0.0363,  ...,  0.0032, -0.0540, -0.0414]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "processor = AutoProcessor.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "model = AutoModel.from_pretrained(\"koclip/koclip-base-pt\")\n",
    "url = \"/Users/moohosong/Desktop/Repos/patent_search/frontend/image/2023-11-28T11_43_05.562722.jpg\"\n",
    "image = Image.open(url)\n",
    "text = [\"소파 와 고양이\", \"강아지와 강아지 주인\", \"쳇바퀴를 달리는 햄스터\", \"자동차\", \"뭘봐\"]\n",
    "image\n",
    "#Load CLIP model\n",
    "# model = SentenceTransformer('clip-ViT-B-32')\n",
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "inputs = processor(\n",
    "    text=text,\n",
    "    images=image, \n",
    "    return_tensors=\"pt\", # could also be \"pt\" \n",
    "    padding=True\n",
    ")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "probs = softmax(outputs.logits_per_image)\n",
    "\n",
    "for idx, prob in sorted(enumerate(*probs), key=lambda x: x[1], reverse=True):\n",
    "    print(text[idx], prob)\n",
    "    \n",
    "te = outputs.text_embeds\n",
    "ie = outputs.image_embeds\n",
    "\n",
    "#Encode an image:\n",
    "# img_emb = model.encode(Image.open('two_dogs_in_snow.png')) # or jpg\n",
    "\n",
    "#Encode text descriptions\n",
    "# text_emb = model.encode(['두 개가 눈 위에 있다.', '고양이가 책상에 앉아있다.', '저녁 런던의 사진'])\n",
    "\n",
    "#Compute cosine similarities \n",
    "# cos_scores = util.cos_sim(img_emb, text_emb)\n",
    "# print(cos_scores)\n",
    "print(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50, 768])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vo = vision_output[0]\n",
    "vo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1024])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to = text_output[1]\n",
    "to.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ieo = image_embeds_output\n",
    "ieo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = outputs.text_embeds\n",
    "te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ie = outputs.image_embeds\n",
    "ie.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3001, 0.1135, 0.1890, 0.2055, 0.1919], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dot = (ie * te).sum(dim = -1)\n",
    "softmax0 = torch.nn.Softmax(dim = 0)\n",
    "print(softmax0(dot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3001, 0.1135, 0.1890, 0.2055, 0.1919], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "normed_dot = dot / (torch.norm(te, dim = -1) * torch.norm(ie, dim = -1)).reshape(-1)\n",
    "print(softmax0(normed_dot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have to specify pixel_values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\n\u001b[1;32m      2\u001b[0m     text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m      3\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# could also be \"pt\" \u001b[39;00m\n\u001b[1;32m      4\u001b[0m     padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p4ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p4ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p4ds/lib/python3.10/site-packages/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py:357\u001b[0m, in \u001b[0;36mVisionTextDualEncoderModel.forward\u001b[0;34m(self, input_ids, pixel_values, attention_mask, position_ids, return_loss, token_type_ids, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39m>>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mreturn_dict\n\u001b[0;32m--> 357\u001b[0m vision_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(\n\u001b[1;32m    358\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m    359\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    360\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    361\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    362\u001b[0m )\n\u001b[1;32m    364\u001b[0m text_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_model(\n\u001b[1;32m    365\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    366\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    371\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    372\u001b[0m )\n\u001b[1;32m    374\u001b[0m image_embeds \u001b[39m=\u001b[39m vision_outputs[\u001b[39m1\u001b[39m]  \u001b[39m# pooler_output\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p4ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p4ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p4ds/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:918\u001b[0m, in \u001b[0;36mCLIPVisionModel.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    895\u001b[0m \u001b[39mReturns:\u001b[39;00m\n\u001b[1;32m    896\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[39m>>> pooled_output = outputs.pooler_output  # pooled CLS states\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[39m```\"\"\"\u001b[39;00m\n\u001b[1;32m    916\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 918\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvision_model(\n\u001b[1;32m    919\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m    920\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    921\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    922\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    923\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p4ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p4ds/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/p4ds/lib/python3.10/site-packages/transformers/models/clip/modeling_clip.py:841\u001b[0m, in \u001b[0;36mCLIPVisionTransformer.forward\u001b[0;34m(self, pixel_values, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    838\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    840\u001b[0m \u001b[39mif\u001b[39;00m pixel_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou have to specify pixel_values\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    843\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(pixel_values)\n\u001b[1;32m    844\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_layrnorm(hidden_states)\n",
      "\u001b[0;31mValueError\u001b[0m: You have to specify pixel_values"
     ]
    }
   ],
   "source": [
    "inputs = processor(\n",
    "    text=text,\n",
    "    return_tensors=\"pt\", # could also be \"pt\" \n",
    "    padding=True\n",
    ")\n",
    "outputs = model(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('p4ds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f2de807a89c7fb575bc4374298399e6d8b40e6b5278db170b5a83a47030725c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
