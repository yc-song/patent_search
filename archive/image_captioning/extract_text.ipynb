{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "\n",
    "import tiktoken\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.memory import ConversationBufferMemory\n",
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain.llms import HuggingFaceHub\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from textutils import extract_ref_from_text, get_number2drawing_dict, convert_refs_to_drawing_num\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 999)\n",
    "os.chdir('/Users/hayley/Documents/p4ds/patent_search')\n",
    "load_dotenv() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py 돌려서 나온 결과 다시 저장\n",
    "# sample_data = pd.read_csv('data_preprocess/data.csv')\n",
    "# sample_data.to_excel(\"data_preprocess/data.xlsx\")\n",
    "\n",
    "# 근데 코드에서 코너 케이스 때문에 처리가 안된 pdf 가 있어서 그거는 excel 파일 다운받아서 눈으로 보고 채움 -> 그러다보니까 \\n이 안지워진 경우가 있어서 지웠음\n",
    "# sample_data = pd.read_excel('data_preprocess/data.xlsx')\n",
    "# for col in sample_data.columns:\n",
    "#     sample_data[col] = sample_data[col].str.replace('\\n', '')\n",
    "# sample_data.to_excel(\"data_preprocess/data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## negative sample data 추가\n",
    "# sample_data = pd.read_excel(\"data_preprocess/sample_data_old.xlsx\", index_col=0)\n",
    "# negative_samples = pd.read_csv('pdf_process/data.csv')\n",
    "# negative_samples['id'] = negative_samples['id'].astype(str)\n",
    "# negative_samples = negative_samples.loc[negative_samples['id']!= '1020160014413 (1)']\n",
    "    \n",
    "# negative_samples = negative_samples.loc[~negative_samples['id'].isin(sample_data['id'])]\n",
    "# sample_data = pd.concat([sample_data, negative_samples], axis=0)\n",
    "\n",
    "# # add labels column\n",
    "# sample_data['labels'] = \"\"\n",
    "# sample_data.loc[sample_data['id']=='1020180014052', 'labels'] = 'source'\n",
    "# sample_data.loc[sample_data['id'].isin(['1020050097605','1020177009557', '1020120156759']), 'labels'] = 'target'\n",
    "# sample_data['labels'] = sample_data['labels'].fillna(\"negative\")\n",
    "# sample_data.to_excel(\"data_preprocess/sample_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pd.read_excel(\"data_preprocess/sample_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "**logs**\n",
    "\n",
    "23.11.15\n",
    "\n",
    "- 1) problem with extracting reference (number) from texts -> some reference codes are not just numbers. it's number + alphabet e.g. 202a, 202b, or sometimes just uppercase alphabets e.g. T, SR~SZ\n",
    "- 2) so I tried to first extract the reference codes from \"부호에 대한 설명\" column. (Because then I can find those specific reference codes inside of texts.) But extracting reference codes from \"부호에 대한 설명\" was difficult because currently all \\<code\\>: \\<description\\> string pairs are concatenated without \\n and the parsing reference codes by just relying on regular expression was almost impossible. (some reference codes were uppercase alphabets, but including uppercase alphabets for codes caused problems.) So I asked Mooho if he could leave \\n characters left for \"부호에 대한 설명\" section. He said yes, and I paused developing regex rules further.  \n",
    "\n",
    "23.11.16\n",
    "- I proceeded with some noise in reference extraction. \n",
    "- I also added some negative samples (3 random samples that are not prior arts of the source patent.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = pd.read_excel('data_preprocess/sample_data.xlsx',index_col=0)\n",
    "\n",
    "## image to numbers, numbers to image\n",
    "num2drawing_dicts = get_number2drawing_dict('/Users/hayley/Documents/p4ds/patent_search/data_preprocess/mock_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c.f. hupd에서 쓴 컬럼들 \n",
    "#     \"abstract\": \"...\", # 요약 -> 우리나라 특허에는 abstract가 따로 있는거 같진 않고 요약 1개 섹션임\n",
    "#     \"claims\": \"...\", # 청구범위\n",
    "#     \"background\": \"...\", # 기술분야 + 배경기술\n",
    "#     \"summary\": \"...\", # 요약? \n",
    "#     \"full_description\": \"...\" # 해결하려는과제 + 과제의해결수단 + 발명의효과 + 발명을실시하기위한구체적인내용???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.columns \n",
    "# text embedding 대상이 되는 컬럼\n",
    "# '요약', '청구범위', '기술분야', '배경기술', '해결하려는과제', '과제의해결수단',\n",
    "#        '발명의효과', '도면의간단한설명', '발명을실시하기위한구체적인내용', '부호의설명'\n",
    "\n",
    "# 각 섹션을 따로 따로 임베딩하는게 나을지 아니면은, 몇 섹션은 합치는게 나을지 고민이다. -> 일단 빠르게 ㄱㄱ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['요약', '청구범위', '기술분야', '배경기술', '해결하려는과제', '과제의해결수단',\n",
    "       '발명의효과', '도면의간단한설명', '발명을실시하기위한구체적인내용', '부호의설명']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # c.f. 컬럼별 길이 분포 \n",
    "# all_data = pd.read_excel('/Users/hayley/Documents/p4ds/patent_search/pdf_process/data_large.xlsx', index_col=0)\n",
    "# for col in all_data.columns:\n",
    "#     try:\n",
    "#         if \"extracted_numbers\" not in col:\n",
    "#             lens = all_data[col].apply(lambda x: len(str(x)))\n",
    "#             print(col, lens.mean().round(2))\n",
    "#     except:\n",
    "#         continue\n",
    "# # id 13.04\n",
    "# # 요약 301.69\n",
    "# # 대표도 395.03\n",
    "# # 청구범위 3565.65\n",
    "# # 기술분야 150.39\n",
    "# # 배경기술 1713.51\n",
    "# # 해결하려는과제 210.42\n",
    "# # 과제의해결수단 1653.4\n",
    "# # 발명의효과 209.81\n",
    "# # 도면의간단한설명 1206.95\n",
    "# # 발명을실시하기위한구체적인내용 17958.91\n",
    "# # 부호의설명 188.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patent_chunk_dicts=[]\n",
    "\n",
    "for i, row in sample_data.iterrows():\n",
    "    patent_dict = dict( \n",
    "        # 특허 번호 따기\n",
    "        application_number = str(row.id), # 출원 번호\n",
    "        publication_number = '', # 공개 번호\n",
    "        patent_number = '', # 등록 번호\n",
    "        chunks = [],\n",
    "        # chunks_wo_drawing_and_numbers_desc = []\n",
    "    )\n",
    "    print(f\"patent {row.id} 's chunk sizes\")\n",
    "    \n",
    "    # text column들을 돌면서, chunking 하기\n",
    "    chunks = []\n",
    "    drawing_nums_list = []\n",
    "    refs_list = []\n",
    "    # chunks_wo_drawing_desc = [] # 도면의 간단한 설명, 부호의 설명 제외.. false positive 가 생기지는 않을까?\n",
    "    for col in text_columns:\n",
    "        if (col in ['도면의간단한설명', '부호의설명']) or (str(row[col]) == 'nan'):\n",
    "            continue\n",
    "        curr_section_chunks = text_splitter.create_documents([str(row[col])], [{\"application_number\": str(row.id)}])\n",
    "        chunks.extend(curr_section_chunks)\n",
    "        \n",
    "        # chunks_wo_drawing_desc.extend(curr_section_chunks)\n",
    "        print(col, len(curr_section_chunks), end=' | ')\n",
    "        \n",
    "        # chunk 별로 reference 발생한 부호 찾기\n",
    "        for chnk in curr_section_chunks:\n",
    "            refs = extract_ref_from_text(chnk)\n",
    "            refs_list.append(refs)\n",
    "            \n",
    "            #  부호가 있었으면 drawing number 로 한번 또 치환하기\n",
    "            if len(refs) > 0: # if not empty, convert found references to drawing numbers\n",
    "                drawing_nums = convert_refs_to_drawing_num(refs, num2drawing_dicts[str(row.id)]['num2drawing'])\n",
    "            else:\n",
    "                drawing_nums = []\n",
    "            drawing_nums_list.append(drawing_nums)\n",
    "                \n",
    "        \n",
    "    # patent_dict['chunks'] = list(zip(chunks, zip(refs_list, drawing_nums_list)))\n",
    "    patent_dict['chunks'] = chunks\n",
    "    # patent_dict['chunks_wo_drawing_and_numbers_desc'] = chunks_wo_drawing_desc\n",
    "    print()\n",
    "    patent_chunk_dicts.append(patent_dict)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(patent_chunk_dicts, open('data_preprocess/sample_chunk_data.json', 'w'), ensure_ascii = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in select_columns:\n",
    "#     sample_data[f\"{col}_ref\"] = sample_data[col].apply(extract_ref_from_text)\n",
    "\n",
    "# sample_data['도면의간단한설명_dict'] = sample_data['도면의간단한설명'].apply(extract_description_for_image)\n",
    "\n",
    "# sample_data.loc[:,'부호의설명_dict'] = sample_data['부호의설명'].apply(extract_description_for_code)\n",
    "\n",
    "# for col in select_columns:\n",
    "#     sample_data[f\"{col}_ref_drawing\"] = sample_data.apply(convert_ref_to_drawing_num, args=(col,image_df), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR\n",
    "\n",
    "- Tesseract : 잘 못함\n",
    "- 다른 오픈소스 : pyocr -> tesseract랑 똑같음 / calamari-ocr -> tensorflow 설치해야 되고 등등 maintain이 잘 안되는 패키지 인듯\n",
    "- GCP vision api : 일단 ui 에서 테스트 해보고 copy json output 버튼 눌러가지고 json output 복사해와서 json output에서 텍스트만 파싱하는 것만 짬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "## tesseract를 먼저 다운받고, 다운받은 경로를 넣어주어야 함.\n",
    "## You need to first download tesseract & insert the path to the exe file below.\n",
    "pytesseract.pytesseract.tesseract_cmd = r'/opt/homebrew/Cellar/tesseract/5.3.3/bin/tesseract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text image to string\n",
    "from glob import glob\n",
    "\n",
    "image_paths = glob('data_preprocess/image/*/*.png')\n",
    "\n",
    "for img in image_paths:\n",
    "    print(img)\n",
    "    print(pytesseract.image_to_string(Image.open(img), lang='eng'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in json_output['fullTextAnnotation']['pages'][0]['blocks']:\n",
    "    for j in i['paragraphs']:\n",
    "        for k in j['words']:\n",
    "            curr_word = ''\n",
    "            for l in k['symbols']:\n",
    "                curr_word += l['text']\n",
    "            if curr_word.startswith('-'):\n",
    "                curr_word = curr_word[1:]\n",
    "            print(curr_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json_output['textAnnotations'][0]['description'].split('\\n')\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = json_output['textAnnotations'][0]['description'].split('\\n')\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p4ds-patent-search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
